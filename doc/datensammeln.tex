\chapter{Datenverarbeitung}

Wie bei jeder Anwendung von maschinellen Lernverfahren sind die zugrundeliegenden Daten von äußerster Wichtigkeit.
Im Rahmen dieser Masterarbeit wurden zweierlei Sorten von Daten benutzt:
Einmal wurden am \textit{TableSort} Schüttgutsortierer des Fraunhofer IOSBs Aufnahmen gemacht, 
die dann über mehrere Arbeitsschritte in das richtige Datenformat übersetzt wurden.
Zudem existieren der DEM Datensatz \todo{mehr details - Quellen lesen}



\section{Datenformatierung}


\begin{itemize}
	\item Zu Beginn des Datenverarbeitungskapitel erstmal definieren wie unsere Feature-Label Paare aussehen.
	\item Features eigentlich immer gleich:
	\item die Positionen der letzten \(n\) Zeitschritte (FeatureSize Hyperparameter)
	\item also ein \(2n\) Tupel, mit jeweils \(n\) X-Koordinaten und \(n\) Y-Koordinaten

\end{itemize}


Labels: Unterscheidung nach Anwendung:

NextStep: Label ist 2-Tupel, X und Y Koordinate
Separator: 
	gegeben ist eine Stelle entlang der Bewegungsrichtung der Teilchen an der der Separator angebracht ist.
	erstes element des Label ist die Koordinate entlang der orthogonalen Achse zur Bewegungsrichtung wo das Teilchen den Separator passiert
	zweites Element ist die Anzahl von Zeitschritten , die das Teilchen noch bis zum Separator braucht.

Important Point: Labels wurden normalisiert und Standardisiert ( \(\frac{TrueVal - Mean}{Standard Diviation}\)
um auszugleichen, dass sich Position und Zeitschritte auf unterschiedlichen Skalen bewegen und dementsprechend unterschiedlich hohe gradienten haben.


Es ist implementiert, dass die verschiedenen Dimensionen unterschiedlich stark gewichtet werden können - Je nach Schüttgut/präzision des Separators
Aber für die evaluierung ist keine Gewichtung vorgenommen worden.

optional: Histogramme über die Daten (mehr Teilchen in der Mitte bei Location...)



\section{Eigene Aufnahmen}

\subsection{Versuchsaufbau}

\begin{itemize}
	\item Am TableSort System, einmal Band, einmal Rutsche
	\item Beschreibung von der Bonito Kamera, stats usw.
	\item Umrechengröße pixel zu mm
	\item Bandgeschwindigkeit
\end{itemize}

\color{blue}
Zur Aufnahme der Daten wurde eine Bonito CL-400 200 FPS Kamera benutzt, die in Abbildung \ref{pictureCam} zu sehen ist.
Die ist, wie in Abbildung \ref{fig:tablesortsystem} oberhalb des Förderbandes angebracht.
Die Bilder, die von der Kamera aufgenommen werden, haben eine Auflösung von 2320x1726 Pixeln \cite{alliedvisiontechnologiesgmbh2014}.
\todo[inline]{Umrechengröße pixel zu mm, im weiteren verlauf werden pixel benutzt}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{img/banner-Bonito_cropped}
    \caption{Zur Aufnahme verwendete Kamera [TODO: Quelle Bild]}
    \label{pictureCam}
\end{figure}

\subsection{Schüttgüter-Typen}

Aufgenommen wurden vier verschiedene Schüttgüter, die in Abbildung \ref{fig:schuettgueterSchuessel} zu sehen sind.

\begin{itemize}
    \item Kugeln
    \item grüne Pfefferkörner
    \item Zylinder
    \item Weizenkörner
\end{itemize}

Die Kugeln und der Pfeffer sowie die Zylinder und die Weizenkörner bilden jeweils 
ein Paar aus einem geometrischen Körper und einem echten Objekt, das grob dessen Form ähnelt.

Die Kugeln bestehen aus Holz und haben einen Durchmesser von 5mm.
Die Zylinder bestehen ebenfalls aus Holz. Sie haben eine Länge von 1cm und einen Durchmesser von 3mm.
Die Schüttgüter sind in Abbildung \ref{fig:schuettgueterSchuessel} in Schüsseln 
und in Abbildung \ref{fig:schuettgueterBand} auf dem Förderband zu sehen.
\todo{TODO: Details}

\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{kugel_001_00084_debayer}
		\caption{Kugeln}
	\end{subfigure}
	\quad
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{Pfeffer_003_00020_debayer}
		\caption{Pfeffer}
	\end{subfigure}
	\vskip\baselineskip
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{zylinder_001_00009_debayer}
		\caption{Zylinder}
	\end{subfigure}
	\quad
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{weizen_004_00016_debayer}
		\caption{Weizen}
	\end{subfigure}
	\caption{Verschiedene Schüttgüter auf dem Förderband}
	\label{fig:schuettgueterBand}
\end{figure}

\color{black}
\section{Datenpipeline}

\begin{itemize}
	\item Beschreiben wie aus den Bildern die relevanten Features extrahiert werden.
	\item Ursprungs: Bayer Matrix Bitmap
	\item Konvert to RGB
	\item Segmentierungsskript zu CSV No.1
	\item TrackSort Algorithmus zuweisung zu CSV No.2
	\item Das ist dann der finale Punkt von wo es in meinen Code geladen wird und der rest dort passiert 
\end{itemize}

\color{blue}
Die Bonito Kamera nimmt Bilder in Form einer Bayer-Matrix auf, wie sie in \ref{fig:bayerPattern} zu sehen ist.
Diese werden dann in Batches von je 3500 gesammelt und in Bitmap Dateien geschrieben.
\todo{hier Bayer-Matrix erklären und Bild?}

\begin{figure}
	% \missingfigure{bayer matrix}
	\includegraphics[width=0.6\textwidth]{1024px-Bayer_pattern_on_sensor}
	\caption{Bayer Matrix [TODO: Quelle]}
	\label{fig:bayerPattern}
\end{figure}

Auf Grund der Menge an Bildern wurden die Bilder zunächst in das png Dateiformat übertragen.
Die Features, die für das Trainieren der Netze benutzt werden, sind die Koordinaten der Mittelpunkte der Objekte.
Um diese zu bestimmten, müssen zunächst die Dateien mittels \textit{demosaicing} rekonstruiert werden um gewöhnliche RGB Bilder zu erhalten.
Die Open Source Computer Vision Library OpenCV hat eine Methode implementiert, die ein Bild von einem Farbraum in einen anderen übertragen kann.
Diese wurde eingesetzt um die einzelnen Bilder in RGB Farbbilder zu konvertieren.
\todo[inline]{Skript ursprünglich von Georg, ein paar changes implementiert (bezüglich input und output.)}

\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{KugelnCropped}
		\caption{Kugeln}
	\end{subfigure}
	\quad
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{PfefferCropped}
		\caption{Pfeffer}
	\end{subfigure}
	\vskip\baselineskip
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{ZylinderCropped}
		\caption{Zylinder}
	\end{subfigure}
	\quad
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{WeizenCropped}
		\caption{Weizen}
	\end{subfigure}
	\caption{Verschiedene gesammelte Schüttgüter}
	\label{fig:schuettgueterSchuessel}
\end{figure}


Auf diesen kann dann eine Segmentierung vorgenommen werden.
Hierzu wurde erneut die Computer Vision Library OpenCV benutzt.
Für jede Sorte von Schüttgut wurde ein eigenes Parameterprofil von Hand angepasst.
Diese bestehen aus einem oberen und unteren Grenzwert in jedem Kanal des HSV-Raums und einer minimalen Fläche, die ein Teilchen umfassen muss.
Entsprechend der durch das Profil festgelegten Parameter werden für die einzelnen Bilder Masken angelegt,
ob die HSV-Werte der einzelnen Pixel innerhalb oder außerhalb der Grenzwerte liegen. 
Mit diesen Masken werden dann alle möglichen Konturen von Schüttgutpartikeln extrahiert, 
bevor diese nocheinmal bezüglich ihrer Sinnhaftigkeit gefiltert werden. 
Im letzten Schritt wird nun der gewichtete Mittelpunkt der einzelnen Konturen bestimmt und abgespeichert.
Das Ergebnis von diesem Segmentierungsscripts ist ein CSV File für jedes Batch.
Eine Zeile repräsentiert ein Bild aus dem Batch, also einen Zeitschritt.
\todo{CSV Tabelle mit State der da rauskommt}
Zu Beginn jeder Zeile steht zunächst die Frame Nummer, gefolgt von der Anzahl der detektierten Partikel
und den X- und Y-Koordinaten der detektierten Partikel.
\todo{TODO: Verifizieren}
Die Mittelpunkte in diesem CSV File werden nun mittels des in MATLAB implementierten TrackSort Algorithmus einzelnen Tracks zugeordnet,
\todo{Mehr details: Tracksort trackzuweisung Assignment Problem. Referenz Tobi MA?}
die dann wiederum in einem neuen CSV File gespeichert werden.
Die einzelnen Tracks werden als Spaltenpaare dargestellt mit jeweils einer Spalte für die X- und Y-Koordinaten zu einem jeweiligen Zeitpunkt.
Ein Ausschnitt aus einer solchen Datei ist in Tabelle \ref{tableTracks} zu sehen.


\begin{table}[ht]
\caption{Beispielhafter Ausschnitt aus einem CSV File} 
\begin{adjustbox}{width=1\textwidth}
\begin{tabular}{c|c|c|c|c|c}%
    %\bfseries Person & \bfseries Matr.~No.% specify table head
    
    \bfseries TrackID\_1\_X & \bfseries TrackID\_1\_Y & \bfseries TrackID\_2\_X  & \bfseries TrackID\_2\_Y & \bfseries TrackID\_3\_X & \bfseries TrackID\_3\_Y
    \csvreader[head to column names]{docExample.csv}{}% use head of csv as column names
    {\\\hline\csvcoli&\csvcolii&\csvcoliii&\csvcoliv&\csvcolv&\csvcolvi} % specify your coloumns here
    \end{tabular}
\end{adjustbox}

\label{tableTracks}
\end{table} 

\color{black}
\section{Simulierte Daten}

Die DEM Daten, wo sie herkommen, was der unterschied ist zu den selbstaufgenommenen Daten. Vorteile und Nachteile...
\todo[inline]{Quellen}

Original: 1000Hz, downsampled auf 200Hz, 

Nicht ganz so viele Partikel, aber dafür sehr lange tracks - Informationen auf dem gesamten Band, nicht nur auf dem Part wo die Kamera drauf schaut.

Vergleich bezüglich der eignung für die verschiedenen Ansatze dann im Evaluations Kapitel


\subsection{Menge}

Insgesamt wurden 265451 Bilder aufgenommen.
177951 Bilder auf dem Förderband
87500 Bilder auf der Rutsche


Anzahl separate Tracks.
Anmerkung: Simulationsdaten sind die echte Wahrheit, während die werte von den Selbstgesammelten Daten hier 
auf die Korrektheit des Outputs vom Tracksort Algorithmus beruhen. 

Es wurden 
7538 Kugeln in 15 Batches,
7056 Pfefferkörner in 13 Batches,
17049 Zylinder in 11 Batches
und 8549 Weizenkörner in 13 Batches aufgenommen.
\todo[inline]{das in eine Schöne Tabelle stecken und erzählen}

\todo[inline]{Table mit Anzahl von Elementen in verschiedenen Batches}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{img/scaledPieChart-trimmed}
    \caption{Verteilung Schüttgut Elemente nach Sorte}
    \label{piechartSchuettgut}
\end{figure}


% Menge in gecleanten Daten:

% 7343 Kugeln
% 6824 Pfefferkörner
% 15760 Zylinder
% 8426 Weizenkörner


\section{Daten Postprocessing}

Cleanup : FilterTracksByAngle, FilterByVectorLengthChange \todo{Überlegen ob ich da so viel aufschreiben soll - 
minimaler unterschied nach Resegment (Bessere Tracksort zuordnung?)}


\begin{itemize}
	\item Data Augmentation: Definition und Beschreibung
	\item bei Bildern normalerweise Rotieren, Translation, Ausschnitte...
	\item Hier: Spiegeln
	\item in einem Band - an der Mitte, nicht die Ränder mit nehmen - Kamera nicht perfekt zentriert
	\item führt zu: Beinah verdoppelung der Feature-Label-paare fürs training.
\end{itemize}

\color{blue}
Als Data Augmentation bezeichnet man Verfahren, die die eigenen Daten erweitern ohne zusätzliche Daten aufzunehmen.
Man generiert aus den bestehenden Daten zusätzliche, synthetische Daten, die dann im Trainingsset eingesetzt werden können.
Ausreichend viele Trainingsbeispiele zu haben ist notwendig um mit neuronalen Netzen eine gute Performance zu erzielen.
Die synthetischen Beispiele müssen jedoch plausibel sein, da sie sonst die Qualität der Ausgabe des Netzes negativ beeinträchtigen können.

Für Netze, die in der Bilderkennnung eingesetzt werden gibt es einige weit verbreitete Techniken,
zum Beispiel Rotation, Translation, Spiegeln und das Ausschneiden von Teilbildern.

Für den gegebenen Fall mit den Mittelpunkten von Schüttgut Partikeln als Features resultiert von diesen Techniken nur das Spiegeln in sinnvollen Daten.
gespiegelt wird an der Mittellinie entlang der Bewegungsrichtung.
Tracks, die [zu nah an den Rand kommen] werden ausgenommen, da zumindest bei den selbst aufgenommenen Daten, die Kamera nicht perfekt zentriert ist.

\begin{figure}
	\missingfigure{Visualisierung Dataaugmentation}
	% \includegraphics[width=\textwidth]{TrackSortPic}
	\caption{Visualisierung Data Augmentation durch Spiegelung}
	% \todo{Quelle Bild!}
	\label{fig:dataAugm}
\end{figure}


\color{black}
\section{Trainingsbeispiele}

Train - Test - Validation - Split:
Train - test, 90\% zu 10\%.
Validation nur für die sets auf denen das Hyperparameter Tuning gemacht wurde
[ungefähres ]


Features:
NextStep einfach alle \(n\)--Tupel, die ein Track hergibt, sodass es noch ein Label geben würde
Separator: Muss unterschieden werden - Filtern oder nicht filtern, danach ob es das letzte mögliche Tupel vor der prediction Phase ist.
Mit filtern besseres ergebnis, aber auch deutlich weniger Trainingsbeispiele (Overfitting wird mehr zur Gefahr)
Ohne Filtern: Flexibler und mehr Trainingsbeispiele - man könnte im Nachhinen den PredictionCutOff verlegen 
und einfach das Netz weiter verwenden ohne neu zu trainieren.
Maybe ein Mittelding, das man nicht alle tupel nimmt aber auch nicht nur die letzten? Ausblick, zukunft
\todo[inline]{Example: Gegeben ein Track, was für Features würden da rausfallen}


Labels:
Sehr straight forward für NextStep (Literally), einfach die nächste Zeile im Track jeweils für X und Y

für separator slightly more complicated: 
Element des Tracks vor und hinter der Separator position (entlang der Travel Achse)

Schnittpunkt geometrisch bestimmen.
Label für position ist die Position entlang der Achse orthogonal zur Bewegungsrichtung vom Schnittpunkt der Separatorlinie und 
der Strecke zwischen dem Element vor und dem element hinter. siehe \ref{fig:Schnittpunkt}


\begin{figure}
	\missingfigure{geometrische Bestimmung des Schnittpunkts mit dem Separator}
	% \includegraphics[width=\textwidth]{TrackSortPic}
	\caption{geometrische Bestimmung der Labels}
	% \todo{Quelle Bild!}
	\label{fig:Schnittpunkt}
\end{figure}

\todo{table of size of different data sets - number of pictures...}

Verhältnis: Anzahl Feature-Label-Paare für verschiedene Beispiele und verschiedene Settings
(FeatureSize, Filter Ja/Nein, Augmentation Ja/Nein) Als Tabelle?

OUTDATED:
Bei einer FeatureSize von 5 ergeben sich bei den Kugeln so 98.966 Feature-Label Paare.
Die Pfefferkörner haben dann 105.101 Feature-Label Paare,
bei den Zylindern kommt man auf 244.422 Feature-Label Paare
und bei den Weizenkörner 132.140 Feature-Label Paare.
