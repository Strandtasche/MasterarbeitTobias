\chapter{Datenverarbeitung}

% Wie bei jeder Anwendung von maschinellen Lernverfahren sind die zugrundeliegenden Daten von äußerster Wichtigkeit.
Bei maschinellen Lernverfahren ist die Qualität und die Menge der zugrundeliegenden Daten sehr wichtig,
da diese die Güte des Ergebnisses stark beeinflussen können.
Im Rahmen dieser Masterarbeit wurden zweierlei Arten von Daten benutzt:
Einerseits wurden am \textit{TableSort} Schüttgutsortierer des Fraunhofer IOSBs Aufnahmen gemacht, 
die dann über mehrere Arbeitsschritte in das richtige Datenformat übersetzt wurden.
Zudem existieren Datensätze, die durch eine Simulation mittels der \textit{Diskrete-Elemente-Methode} (DEM) entstanden sind. 
In diesem Kapitel soll nun beschrieben werden, woher diese Daten kommen, wie sie verarbeitet werden 
und in welcher Form sie dann letztendlich in die neuronalen Netze gegeben werden.
Abschließend wird die Datenaugmentierung, die vorgenommen wurde, vorgestellt und erklärt.





\section{Eigene Aufnahmen}

In diesem Abschnitt soll beschrieben werden, welche Form die am Fraunhofer IOSB selbst aufgenommenen Daten haben und welche Konfigurationen bezüglich Schüttgut und Sortierer sie darstellen.


% \subsection{Versuchsaufbau}
% \todo[inline]{passendere überschrift finden?}

% \color{blue}
% \begin{itemize}
% 	\item Am TableSort System, einmal Band, einmal Rutsche
% 	\item Beschreibung von der Bonito Kamera, stats usw.
% 	\item Umrechengröße pixel zu mm
% 	\item Bandgeschwindigkeit
% \end{itemize}
% \color{black}


Zur Aufnahme der Daten wurde eine Bonito~CL-400~200~FPS~Kamera benutzt. 
% , die in Abbildung~\ref{pictureCam} zu sehen ist.
Diese wurde, wie in Abbildung~\ref{fig:tablesortsystem} dargestellt, oberhalb des Förderbandes beziehungsweise der Rutsche angebracht.
Die Bilder, die von der Kamera aufgenommen werden, haben eine Auflösung von 2320x1726~Pixeln~\cite{alliedvisiontechnologiesgmbh2014}.
Anhand mehrerer Kalibrierungsbilder wurde bestimmt, dass 1 Pixel im Bild ungefähr \SI{0.056}{\milli\meter} entspricht.
Im weiteren Verlauf der Arbeit wird mit Pixeln~(px) als der Einheit des Orts für die selbst aufgenommenen Bilder gearbeitet.

% \todo[inline]{Rutschenkonfiguration Tablesort ausführlicher erwähnen?}
% \todo[inline]{Umrechengröße pixel zu mm, im weiteren verlauf werden pixel benutzt}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{img/banner-Bonito_cropped}
%     \caption{Zur Aufnahme verwendete Kamera [TODO: Quelle Bild]}
%     \label{pictureCam}
% \end{figure}

% \subsection{Schüttgüter-Typen}

Aufgenommen wurden Daten von vier verschiedenen Schüttgütern: Kugeln, grüne Pfefferkörner, Zylinder und Weizenkörner.
Diese Schüttgüter sind in Abbildung~\ref{fig:schuettgueterSchuessel} in Schüsseln 
und in Abbildung~\ref{fig:schuettgueterBand} während den Aufnahmen zu sehen.

% die in Abbildung~\ref{fig:schuettgueterSchuessel} zu sehen sind.

% \begin{itemize}
%     \item Kugeln
%     \item grüne Pfefferkörner
%     \item Zylinder
%     \item Weizenkörner
% \end{itemize}

Die Kugeln bestehen aus Holz und haben einen Durchmesser von \SI{5}{\milli\metre}.
Die Zylinder bestehen ebenfalls aus Holz. Sie haben eine Länge von \SI{1}{\centi\metre} und einen Durchmesser von \SI{3}{\milli\metre}.
Die Kugeln und der Pfeffer, sowie die Zylinder und die Weizenkörner, bilden jeweils 
ein Paar aus einem geometrischen Körper und einem echten Objekt, das grob dessen Form ähnelt.

% \todo{Mehr Details?}

Alle Schüttgüter wurden auf der Förderbandkonfiguration des \textit{TableSort} Systems aufgenommen.
Zusätzlich dazu wurden Kugeln und Pfefferkörner auf der Rutschenkonfiguration des \textit{TableSort} Systems aufgenommen.

\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{KugelnCropped}
		\caption{Kugeln}
	\end{subfigure}
	\quad
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{PfefferCropped}
		\caption{Pfefferkörner}
	\end{subfigure}
	\vskip\baselineskip
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{ZylinderCropped}
		\caption{Zylinder}
	\end{subfigure}
	\quad
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{WeizenCropped}
		\caption{Weizen}
	\end{subfigure}
	\caption{Verschiedene gesammelte Schüttgüter}
	\label{fig:schuettgueterSchuessel}
\end{figure}

\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{kugel_001_00084_debayer}
		\caption{Kugeln auf dem Förderband}
	\end{subfigure}
	\quad
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{Pfeffer_003_00020_debayer}
		\caption{Pfefferkörner auf dem Förderband}
	\end{subfigure}
	\vskip\baselineskip
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{zylinder_001_00009_debayer}
		\caption{Zylinder auf dem Förderband}
	\end{subfigure}
	\quad
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{weizen_004_00016_debayer}
		\caption{Weizen auf dem Förderband}
	\end{subfigure}
	\vskip\baselineskip
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{kugeln_rutsche_013_00043_debayer}
		\caption{Kugeln auf der Rutsche}
	\end{subfigure}
	\quad
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=\textwidth]{pfeffer_rutsche_005_00056_debayer}
		\caption{Pfefferkörner auf der Rutsche}
	\end{subfigure}
	\caption{Verschiedene Schüttgüter auf dem Förderband bzw. der Rutsche}
	\label{fig:schuettgueterBand}
\end{figure}


\section{Datenpipeline} \label{sec:pipeline}

% \color{blue}
% \begin{itemize}
% 	\item Beschreiben wie aus den Bildern die relevanten Features extrahiert werden.
% 	\item Ursprungs: Bayer Matrix Bitmap
% 	\item Konvert to RGB
% 	\item Segmentierungsskript zu CSV No.1
% 	\item TrackSort Algorithmus zuweisung zu CSV No.2
% 	\item Das ist dann der finale Punkt von wo es in meinen Code geladen wird und der rest dort passiert 
% \end{itemize}
% \color{black}

Die Features, die für das Trainieren der Netze benutzt werden, sind die Koordinaten der Mittelpunkte der Partikel.
Diese aus den aufgenommenen Bildern zu bestimmen, ist die Aufgabe dieser Pipeline.

Am Anfang davon stehen die Bilder, die die Bonito-Kamera in Form einer Bayer-Matrix aufnimmt, wie sie in Abbildung~\ref{fig:bayerPattern} zu sehen ist.
Die Bilder werden in Batches von je 3500 Bildern gesammelt und in Bitmap-Dateien geschrieben.
\todo{überlegen ob ich vielleicht einen Besseren Begriff als batch finde (und wenn nicht besser definieren)}
\todo{Optional: Bayer-Matrix erklären}

\begin{figure}[h]
	% \missingfigure{bayer matrix}
	\centering
	\includegraphics[width=0.6\textwidth]{BayerPattern.pdf}
	\caption{Bayer-Matrix auf einem Sensor\cite{bayerPattern06}}
	\label{fig:bayerPattern}
\end{figure}

Auf Grund der Menge an Bildern und dem damit verbundenen Speicherbedarf wurden die Bilder zunächst in das PNG-Dateiformat übertragen.
Danach müssen zunächst die gebayerten Dateien mittels eines Vorgangs, der als \textit{demosaicing} bezeichnet wird,
als Farbbilder rekonstruiert werden.
Das geschiet mit einem Skript des Frauenhofer IOSB, das die Open Source Computer Vision Library OpenCV \footnote{https://opencv.org/} benutzt.
% Dies geschieht mittels der Open Source Computer Vision Library OpenCV \footnote{https://opencv.org/}, die ein Bild von einem Farbraum in einen anderen übertragen kann.
Dieses Skript wurde angepasst und damit und dann damit die einzelnen Bilder in RGB-Farbbilder konvertiert.
% Ein Skript, das vom Fraunhofer IOSB zur Verfügung gestellt wurde, wurde angepasst und damit die einzelnen Bilder in RGB-Farbbilder konvertiert.
% \todo[inline]{Skript ursprünglich von Georg, ein paar changes implementiert (bezüglich input und output.)}
Auf diesen Bildern kann dann eine Segmentierung vorgenommen werden.
Dafür wurde ebenfalls ein existierendes Skript des Fraunhofer IOSBs angepasst, in dem erneut die Computer Vision Library OpenCV benutzt wird.
Für jede Sorte von Schüttgut wurde ein eigenes Parameterprofil von Hand optimiert.
Ein solches Parameterprofil besteht aus einem oberen und unteren Grenzwert in jedem Kanal des HSV-Raums und einer minimalen Fläche, die ein Teilchen umfassen muss.
Entsprechend der im Profil festgelegten Parameter werden für die einzelnen Bilder Masken angelegt,
die angeben ob die HSV-Werte der einzelnen Pixel innerhalb oder außerhalb der Grenzwerte liegen. 
Mit diesen Masken werden dann alle möglichen Konturen von Schüttgutpartikeln extrahiert. 
Die Konturen werden dann bezüglich der Plausibilität ihrer Größe und Form gefiltert. 
Im letzten Schritt wird nun der gewichtete Mittelpunkt der verbleibenden Konturen bestimmt und abgespeichert.


Das Ergebnis von diesem Segmentierungsskripts ist eine CSV-Datei für jeden Batch.
Ein Beispiel für einen Ausschnitt aus solch einer Datei ist in Tabelle~\ref{table:Segmentierungsscript} zu sehen.
Eine Zeile repräsentiert jeweils ein Bild aus dem Batch, also einen Zeitschritt.
Zu Beginn jeder Zeile steht zunächst die Frame-Nummer, gefolgt von der Anzahl der detektierten Partikel
und den Koordinaten der Mittelpunkte der detektierten Partikel.
Aus den Mittelpunkten in dieser CSV-Datei werden nun mittels des in \textsc{Matlab} implementierten \textit{Multi-Target-Tracking}-Algorithmus die in dem Datensatz vorhandenen Tracks abgeleitet,
% \todo{Mehr details: Tracksort trackzuweisung Assignment Problem. Referenz Tobi MA?}
die dann wiederum in einer neuen CSV-Datei gespeichert werden.
Die einzelnen Tracks werden als Spaltenpaare dargestellt, mit jeweils einer Spalte für die X- und Y-Koordinaten und einer Zeile für jeden Zeitschritt.
Ein Ausschnitt aus einer solchen Datei ist in Tabelle \ref{table:tracksortCSV} zu sehen.
Dies ist der Zustand in dem die Daten dann im Programmcode geladen werden. 

\begin{table}[p]
    \caption{Ausschnitt aus dem Ergebnis des Segmentierungsskripts}
	\small
	\centering
    \begin{tabular}{@{}rcrrrrrr@{}}
    \toprule
    Frame   & \#MP & MP\_1\_x  & MP\_1\_y  & MP\_2\_x  & MP\_2\_y  & MP\_3\_x & MP\_3\_y \\ \midrule
    636     & 1    & 1222.9975 & 92.7641   & NaN       & NaN       & NaN      & NaN      \\
    637     & 1    & 1223.4063 & 182.9758  & NaN       & NaN       & NaN      & NaN      \\
    638     & 1    & 1223.6052 & 273.2425  & NaN       & NaN       & NaN      & NaN      \\
    639     & 1    & 1223.7067 & 364.0339  & NaN       & NaN       & NaN      & NaN      \\
    640     & 1    & 1224.0704 & 453.9057  & NaN       & NaN       & NaN      & NaN      \\
    641     & 2    & 1224.2051 & 544.5191  & 1692.4549 & 43.8822   & NaN      & NaN      \\
    642     & 2    & 1224.5793 & 634.7288  & 1696.6901 & 135.9595  & NaN      & NaN      \\
    643     & 2    & 1224.9082 & 726.0094  & 1700.451  & 229.1195  & NaN      & NaN      \\
    644     & 2    & 1225.2296 & 815.9663  & 1704.1472 & 321.2075  & NaN      & NaN      \\
    645     & 2    & 1225.4286 & 906.7078  & 1708.0593 & 414.2785  & NaN      & NaN      \\
    646     & 2    & 1225.7588 & 996.0286  & 1711.5309 & 506.0545  & NaN      & NaN      \\
    647     & 3    & 1226.0411 & 1086.5729 & 1714.8831 & 599.5417  & 961.8821 & 62.7111  \\
    648     & 3    & 1226.2337 & 1175.9271 & 1718.1401 & 691.6325  & 958.5526 & 154.3124 \\
    649     & 3    & 1226.2073 & 1265.7495 & 1721.6618 & 784.5927  & 955.3107 & 246.5241 \\
    650     & 3    & 1226.2543 & 1354.9362 & 1724.9158 & 876.7192  & 952.4919 & 338.1123 \\
    651     & 3    & 1226.2634 & 1444.5903 & 1728.3341 & 970.2909  & 949.2896 & 430.9692 \\
    652     & 3    & 1226.0845 & 1533.0901 & 1732.1745 & 1062.4624 & 946.3455 & 522.8667 \\
    653     & 3    & 1225.7319 & 1621.8461 & 1735.8759 & 1155.2937 & 943.3384 & 615.4545 \\
    654     & 2    & 1739.6714 & 1247.1867 & 940.2511  & 707.7306  & NaN      & NaN      \\
    655     & 2    & 1743.4279 & 1339.4146 & 937.2216  & 800.4557  & NaN      & NaN      \\
    656     & 2    & 1747.1525 & 1430.2501 & 934.5311  & 891.7249  & NaN      & NaN      \\
    657     & 2    & 1750.9771 & 1521.8102 & 931.6626  & 984.2284  & NaN      & NaN      \\
    658     & 2    & 1754.1491 & 1612.5565 & 928.7587  & 1076.4749 & NaN      & NaN      \\
    659     & 1    & 925.8463  & 1168.794  & NaN       & NaN       & NaN      & NaN      \\
    660     & 1    & 922.8752  & 1260.7461 & NaN       & NaN       & NaN      & NaN      \\
    661     & 1    & 920.2056  & 1352.8549 & NaN       & NaN       & NaN      & NaN      \\
    662     & 1    & 917.4051  & 1444.3431 & NaN       & NaN       & NaN      & NaN      \\
    663     & 1    & 914.6493  & 1535.5131 & NaN       & NaN       & NaN      & NaN      \\
    664     & 1    & 911.8565  & 1626.5341 & NaN       & NaN       & NaN      & NaN      \\ \bottomrule
    \end{tabular}
    \normalsize
    
    \label{table:Segmentierungsscript}
    \end{table}


\begin{table}[p]
	\caption{Ausschnitt aus dem Ergebnis des \textit{Multi-Target-Tracking}-Algorithmus}
	\label{table:tracksortCSV}
    \small
    \centering
    \begin{tabular}{@{}rrrrrr@{}}
    \toprule
    TrackID\_4\_X & TrackID\_4\_Y & TrackID\_5\_X & TrackID\_5\_Y & TrackID\_6\_X & TrackID\_6\_Y \\ \midrule
    1036.4613     & 82.3719       & 1899.9239     & 83.2049       & 1654.4423     & 50.6811       \\
    1033.0189     & 174.9809      & 1896.8142     & 171.3283      & 1655.3193     & 143.9749      \\
    1029.6167     & 266.4979      & 1893.5937     & 259.8098      & 1656.0221     & 237.1573      \\
    1026.3908     & 358.4831      & 1890.3912     & 348.1731      & 1656.8966     & 329.8636      \\
    1023.0203     & 449.6429      & 1887.1035     & 436.4588      & 1657.6308     & 423.1592      \\
    1019.5391     & 542.2334      & 1883.7761     & 525.1073      & NaN           & NaN           \\
    NaN           & NaN           & 1880.2716     & 613.0896      & NaN           & NaN           \\
    NaN           & NaN           & 1876.6054     & 701.9719      & NaN           & NaN           \\
    NaN           & NaN           & NaN           & NaN           & NaN           & NaN           \\ \bottomrule
    \end{tabular}
\end{table}






\section{Simulierte Daten}

% \color{blue}
% Die DEM Daten, wo sie herkommen, was der unterschied ist zu den selbst aufgenommenen Daten. 
% Vorteile und Nachteile...\cite{pieper2016numerical}, \cite{pieper2017numerical} 

% Original: \SI{1000}{\hertz}, downsampled auf \SI{200}{\hertz}, 

% Nicht ganz so viele Partikel, aber dafür sehr lange tracks - Informationen auf dem gesamten Band, nicht nur auf dem Part wo die Kamera drauf schaut.

% Vergleich bezüglich der Eignung für die verschiedenen Ansätze dann im Evaluations Kapitel
% \color{black}

Neben den selbst aufgenommenen Daten wurde im Rahmen dieser Arbeit auch mit einigen existierenden Datensätzen gearbeitet.
Diese Datensätze wurden basierend auf einer hochgenauen numerischen Simulation des \textit{TableSort} Systems mittels der Diskrete-Elemente-Methode erstellt~\cite{pieper2016numerical}\,\cite{pieper2017numerical}.
% \todo{interaktion Teilchen untereinander und mit dem Sortierer?}
In Abbildung~\ref{fig:DEMSimulation} ist die virtuelle Nachbildung des Schüttgutsortierers zu sehen, auf dem die Simulation durchgeführt wurde.
% Die Simulation wird mit einem Zeitschritt von \SI{1e-5}{\second} durchgeführt.
Die Positionsdaten in den Datensätzen waren ursprünglich mit einer Frequenz von \SI{1000}{\hertz} aufgelistet.
Um jedoch Vergleichbarkeit mit den realen Daten zu erhalten wurde diese Frequenz auf \SI{200}{\hertz} reduziert.
Die Positionsangaben der Simulation werden in Metern angeben.
Das Förderband erstreckt sich entlang der y-Achse von \SI{0.0}{\meter} bis \SI{0.18}{\meter} und entlang der x-Achse von \SI{0.388}{\meter} bis \SI{0.788}{\meter} 

Dennoch gibt es einige Unterschiede zwischen den selbst aufgenommenen und den simulierten Daten.
Von diesen Unterschieden ist der schwerwiegendste, dass die Positionen der simulierten Partikel eine absolut verlässliche Ground~Truth sind, 
die direkt aus Simulation entnommen wurde.
So verlässlich sind die Partikelpositionen bei den selbst aufgenommen Daten nicht.
Während den einzelnen in Sektion~\ref{sec:pipeline} beschrieben Schritten kann es zu Fehlern kommen, die sich durch die gesamte Pipeline fortpflanzen.
Die Segmentierung kann an einigen Stellen nicht präzise sein, indem z.\,B. ein Stück Schattens als Teil des Partikels interpretiert wird und dadurch den Mittelpunkt verschiebt. 
Bei einer Kollision von zwei Partikeln kann es dazu kommen, dass der \textit{Multi-Target-Tracking}-Algorithmus die beiden Tracks vertauscht.  
Des Weiteren bewegt sich das Förderband in der Simulation mit \SI{1.5}{\meter\per\second}, 
während Messungen am realen \textit{TableSort} System darauf hindeuten, dass das Förderband dort nur eine Geschwindigkeit von 
circa \SI{1.1}{\meter\per\second} hat.
Ein weiterer Unterschied ist der Aufnahmebereich.
Im Gegensatz zu den selbst aufgenommenen Daten, die nur Informationen 
zu den Bewegungen der Partikel im Bereich auf den die Hochgeschwindigkeitskamera gerichtet ist umfassen, 
befinden sich in den DEM Datensätzen die Positionen der Teilchen über die gesamte Länge des Förderbandes.
Das bedeutet auch, dass in den DEM Datensätzen die Phase der Partikel bevor und während sie vom Förderband beruhigt werden enthalten ist.
Außerdem hat es zur Folge, dass die individuellen Tracks deutlich länger sind, sprich mehr Messungen enthalten.
Bei der NextStep Prädiktion sorgt das dafür, dass es eine deutlich größere Anzahl an Feature-Label-Paaren gibt.
Die Bewegungsrichtung der Partikel ist ebenfalls nicht identisch.
Während die selbst aufgenommenen Datensätze eine Bewegung entlang der y-Achse haben, bewegen sich die simulierten Partikel entlang der x-Achse.
% \todo[inline]{Einheit real: Pixel, Einheit simuliert: m}
\todo[inline]{simulierte haben viel mehr Partikel gleichzeitig auf dem Band - mehr kollisionen...?}

\begin{figure}[h]
    \centering
    % \missingfigure{DEM Simulationsgrafik}
	\includegraphics[width=\textwidth]{DEM-SimOverview}
	\caption{Visualisierung der DEM Simulation, übersetzt~aus~\cite{Pfaff2018}}
	\label{fig:DEMSimulation}
\end{figure}



\section{Datenformatierung}

% \color{blue}
% \begin{itemize}
% 	\item Zu Beginn des Datenverarbeitungskapitel erstmal definieren wie unsere Feature-Label-Paare aussehen.
% 	\item Features eigentlich immer gleich:
% 	\item die Positionen der letzten \(n\) Zeitschritte (FeatureSize Hyperparameter)
% 	\item also ein \(2n\) Tupel, mit jeweils \(n\) X-Koordinaten und \(n\) Y-Koordinaten
% 	% \item \(n\) \textit{n} \(n\) \(n\) \textit{n}
% \end{itemize}

% Labels: Unterscheidung nach Anwendung:

% NextStep: Label ist 2-Tupel, X und Y Koordinate
% Separator: 
% 	gegeben ist eine Stelle entlang der Bewegungsrichtung der Teilchen an der der Separator angebracht ist.
% 	erstes element des Label ist die Koordinate entlang der orthogonalen Achse zur Bewegungsrichtung wo das Teilchen den Separator passiert
% 	zweites Element ist die Anzahl von Zeitschritten , die das Teilchen noch bis zum Separator braucht.

% Important Point: Labels wurden normalisiert und Standardisiert ( \(\frac{TrueVal - Mean}{Standard Diviation}\))
% um auszugleichen, dass sich Position und Zeitschritte auf unterschiedlichen Skalen bewegen und dementsprechend unterschiedlich hohe gradienten haben.


% Es ist implementiert, dass die verschiedenen Dimensionen unterschiedlich stark gewichtet werden können - Je nach Schüttgut/präzision des Separators
% Aber für die evaluierung ist keine Gewichtung vorgenommen worden.

% optional: Histogramme über die Daten (mehr Teilchen in der Mitte bei Location...)
% \color{black}

Das Format, das die Daten annehmen, ist für die NextStep- und den Separator Netze leicht unterschiedlich.
%  unterscheidet sich leicht zwischen den beiden Anwendungen.
Die Feature-Label-Paare der unterschiedlichen Anwendungen unterscheiden sich nur in den Labeln.
Die Eingangsdaten sind -- abhängig von einem Hyperparameter --
in beiden Fällen die Position des Partikels zu den letzten \(n\) Zeitschritten.
Deshalb muss beim Format der Features kein Unterschied zwischen der Art der Aufgabe gemacht werden.
Dieser Hyperparameter wird \textit{FeatureSize} genannt. 
Die Features sind also ein \(2n\)-Tupel, bestehend aus \(n\) X-Koordinaten und \(n\) Y-Koordinaten 
von den Mittelpunkten der \(n\) beobachteten Teilchen.
Die Reihenfolge der Features ist für das neuronale Netz irrelevant, solange sie konsistent zwischen Training und Evaluation bleibt.
Was die Reihenfolge der Features angeht, so ist sie für das neuronale Netz irrelevant, 
solange sie konsistent zwischen Training und Evaluation bleibt. 
In der umgesetzten Implementierung ist es so, dass zuerst die X-Koordinaten 
und dann die Y-Koordinaten in chronologischer Reihenfolge aufgereiht sind. 



Die Labels, die das NextStep-Netz benutzt, sind \(2\)-Tupel.
Diese bestehen aus den X- und Y-Koordinaten des Partikels im nächsten Zeitschritt.
Wie in Abbildung~\ref{fig:FLPNext} dargestellt, befinden diese sich in der nächsten Zeile nach den Features.
% Es handelt sich um die X- und Y-Koordinate der Position des Partikels im nächsten Zeitschritt.
% Diese findet man als die nächste Zeile des Doppelspalte des dazugehörigen Tracks.
% \todo{Grafik um es verständlicher zu machen? Siehe zwischevortrag}

\begin{figure}
	\centering
	% \missingfigure{geometrische Bestimmung des Schnittpunkts mit dem Separator}
	\includegraphics[width=0.6\textwidth]{weizen_003_trackHistory_NothingDeleted-v3}
	\caption[Beispiel Feature-Label-Paar NextStep]{An diesem Ausschnitt eines Tracks wird beispielhaft mit FeatureSize 5 gezeigt, wie die einzelnen Feature-Label-Paare im NextStep Fall entstehen.
		Der rote Rahmen markiert die 10 Features. Der blaue Rahmen markiert das dazugehörigen Label. 
		% Der blaue und der rote Rahmen werden parallel entlang des Tracks verschoben um alle möglichen Feature-Label-Paare zu erhalten. 
		}
	\label{fig:FLPNext}
\end{figure}


Die Labels des Separator-Netzes sind ebenfalls \(2\)-Tupel.
Im Gegensatz zu denen des NextStep-Netzes können diese nicht direkt aus der CSV-Datei ausgelesen werden, da sie nicht gemessen werden.
Stattdessen müssen sie berechnet werden.
Dies ist in Abbildung~\ref{fig:Schnittpunkt} dargestellt.
Für jedes Partikel müssen die letzte Position vor dem Überqueren des Druckluftdüsenarrays \(f_n\) 
und die erste Position nach dem Überqueren \(f_{n+1}\) bestimmt werden.
In Abbildung~\ref{fig:FLPSep} sind dies die blau markierten Einträge. 
Dabei wird die angenommen, dass die Geschwindigkeit des Partikels zwischen \(f_n\) und \(f_{n+1}\) konstant ist.
Dies ist eine Approximation, die ebenfalls in~\cite{Pfaff2018} verwendet wird.
Das erste Element eines jeden Labels ist die Koordinate entlang der Achse orthogonal zur Bewegungsrichtung des Förderbandes, wo das Partikel den Druckluftdüsenarray passiert.
\todo{Bewegungsrichtung Band in der Grafik klarer machen}
Diese erhalten wir, indem wir den Schnittpunkt \(s\) zwischen der Strecke \(f_n\) nach \(f_{n+1}\) und 
der Gerade des Druckluftdüsenarray bestimmen.
Das zweite Element ist die Zeit, die das Partikel noch brauchen wird, bis es das Druckluftdüsenarray passiert.
Sie wird in der Einheit Frames angegeben.
Die ganzzahlige Komponente hiervon ist durch das Zählen der Messungen im Track zu bestimmen.
Die Nachkommastelle wird bestimmt als das Verhältnis von der Distanz zwischen \(f_n\) und \(s\), 
und der Distanz zwischen \(f_n\) und \(f_{n+1}\).
% \todo[inline]{Example: Gegeben ein Track, was für Features und Labels würden da rausfallen}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{geometrie}
	\caption{Geometrische Bestimmung der Labels, nach \cite{Pfaff2018}}
	% \todo{Quelle - test?}
	\label{fig:Schnittpunkt}
\end{figure}

\begin{figure}[h]
	\centering
	% \missingfigure{geometrische Bestimmung des Schnittpunkts mit dem Separator}
	\includegraphics[width=0.6\textwidth]{weizen_003_trackHistory_NothingDeleted_sep-v3}
	\caption[Beispiel Feature-Label-Paar Separator]{An diesem Ausschnitt eines Tracks wird beispielhaft mit FeatureSize 5 gezeigt, wie die einzelnen Feature-Label-Paare im Separator Fall entstehen.
	Analog zu Abbildung~\ref{fig:FLPNext} ist ein Featurevektor markiert. Der Unterschied besteht in der Bestimmung der Labels. 
	Der Schnittpunkt mit dem Druckluftdüsenarray befindet sich zwischen den beiden blau markierten Messungen und kann dementsprechend nicht direkt abgelesen werden.}
	% \todo{Quelle - test?}
	\label{fig:FLPSep}
\end{figure}


% Um zu verhindern, dass verschiedene Label-Elemente basierend auf ihrer Skalierung beim Training unterschiedlich stark gewichtet werden, werden sie standardisiert.
Damit die Fehler der beiden Labelelemente sich ähnlich auf den Gesamtgradienten auswirken,
selbst wenn ihre Werte in sehr unterschiedliche Wertebereichen liegen oder einfach unterschiedliche Einheiten haben, werden die Labels komponentenweise standardisiert.
Dieses Problem würde zum Beispiel auftreten, wenn sich die Werte des einen Elements im Bereich zwischen 0 und 1700 bewegen, während die des Anderen zwischen 1 und 25 liegen,
so wie es bei den Separator Netzen der Fall sein kann.
Bei dem NextStep Fall wäre die Standardisierung nicht zwingend notwendig, 
sie wurde allerdings, um die Einheitlichkeit zu wahren, für beide Fälle implementiert. 
Für die Standardisierung wird von der jeweiligen Komponente der Mittelwert und die Standardabweichung berechnet. 
Im Anschluss wird vom jeweiligen Eintrag \(X\) der zugehörige Mittelwert \(\mu\) subtrahiert, und durch die korrespondierende Standardabweichung \(\sigma\) dividiert
% Jedes Labelelement \(e\), sei es das erste oder das zweite Element seines entsprechenden Tupels, wird gemäß seiner Position vor dem Training angepasst,
% indem davon der Durchschnitt aller ersten beziehungsweise zweiten Elemente abgezogen wird und dann durch die Standardabweichung aller ersten beziehungsweise zweiten Elemente geteilt wird.
\todo[inline]{ist das hier verständlich?}
	
\begin{equation*}
	X_{\text{neu}} = \frac{X - \mu}{\sigma} %\mu_{\text{N}}}{\sigma_{\text{N}}}
\end{equation*}

Durch die Standardisierung wird jedes Label so skaliert, dass die beiden Elemente der Labels jeweils einen Erwartungswert von 0.0 und eine Standardabweichung von 1.0 haben.
Die Ausgaben des Netzes müssen umkehrt zurück in das ursprüngliche Format umgerechnet werden.
% \todo{überlegen wie ich klar mache, dass es sich um den durchschnitt und die Abweichung der jeweiligen Spalte handelt}

% \todo[inline]{Der Absatz hier nach Implementierung verschieben?}
Im Code gibt es die Möglichkeit, unterschiedliche Label-Elemente unterschiedlich zu gewichten, indem Weighted Mean Squared Error als Fehlerfunktion verwendet werden kann. 
Von dieser Möglichkeit wird in den später vorgestellten Ergebnissen nicht Gebrauch gemacht, beziehungsweise den beiden Elementen wird jeweils das Gewicht~1.0 zugewiesen.
Diese Option könnte nützlich sein, falls die Ansteuerung der Druckluftdüsen eine maximale Auflösung hat, jenseits derer eine bessere Zeitprädiktion keine Verbesserung der Sortierqualität mehr erzielt.
% \todo{hier noch schreiben, warum man das tun wollen könnte, oder nicht?}





% Menge in gecleanten Daten:

% 7343 Kugeln
% 6824 Pfefferkörner
% 15760 Zylinder
% 8426 Weizenkörner


\section{Datenaugmentierung}

% \color{blue}

% Cleanup : FilterTracksByAngle, FilterByVectorLengthChange \todo{Überlegen ob ich da so viel aufschreiben soll - 
% minimaler unterschied nach Resegment (Bessere Tracksort zuordnung?)}

% \begin{itemize}
% 	\item Data Augmentation: Definition und Beschreibung
% 	\item bei Bildern normalerweise Rotieren, Translation, Ausschnitte...
% 	\item Hier: Spiegeln
% 	\item in einem Band - an der Mitte, nicht die Ränder mit nehmen - Kamera nicht perfekt zentriert
% 	\item Band zentrierung filter
% 	\item führt zu: Beinah Verdoppelung der Feature-Label-paare fürs training.
% \end{itemize}
% \color{black}

Als Datenaugmentierung oder auch Data Augmentation bezeichnet man Verfahren, die dem eigenen Datenset Datenpunkte hinzufügen ohne zusätzliche Daten aufzunehmen.
Man generiert aus den bestehenden Daten zusätzliche, synthetische Daten, die dann im Trainingsset eingesetzt werden können.
Ausreichend viele Trainingsbeispiele zu haben ist notwendig, um mit neuronalen Netzen eine gute Performance zu erzielen.
Die synthetischen Beispiele müssen jedoch konsistent mit den Originaldaten sein, da sie sonst die Qualität der Ausgabe des Netzes negativ beeinträchtigen können.

Für Netze, die in der Computer Vision eingesetzt werden gibt es einige weit verbreitete Techniken,
zum Beispiel Rotation, Translation, Spiegeln und das Ausschneiden von Teilbildern.

Für den gegebenen Fall mit den Mittelpunkten von Schüttgut-Partikeln als Features resultiert von diesen Techniken nur das Spiegeln in sinnvollen Daten.
Eine beispielhafte Darstellung der vorgenommenen Data Augmentation ist in Abbildung~\ref{fig:dataAugm} zu finden.
Gespiegelt wird an der Mittellinie des Bands, beziehungsweise der Rutsche entlang der Bewegungsrichtung.
Tracks, die eine gewisse Distanz zur Spiegelachse überschreiten, werden ausgenommen, da zumindest bei den selbst aufgenommenen Daten 
die Kamera nicht perfekt zentriert ist.
Deshalb könnte nicht ausgeschlossen werden, 
dass unrealistische Feature-Label-Paare dem Trainingsset hinzugefügt werden würden.

Die Anwendung dieser Technik auf die vorhandenen Daten führt beinahe zu einer Verdoppelung 
der benutzbaren Feature-Label-Paare im Trainingsset, wovon man sich einen positiven Effekt auf die Qualität der Ergebnisse verspricht.

\begin{figure}[h]
	% \missingfigure{Visualisierung Dataaugmentation}
	\centering
	\includegraphics[width=\textwidth]{augmentationImage.pdf}
	\caption{Visualisierung der Datenaugmentierung durch Spiegelung}
	% \todo{Quelle Bild!}
	\label{fig:dataAugm}
\end{figure}



\section{Datenumfang}

Im Rahmen dieser Arbeit wurden insgesamt 247\,951 Bilder aufgenommen.
Davon waren 177\,951 Bilder auf dem Förderband und 70\,000 Bilder auf der Rutsche.
Die Verteilung dieser Bilder ist in Abbildung~\ref{fig:barPics} dargestellt.

\begin{figure}[h]
	% \missingfigure{Visualisierung Dataaugmentation}
	\centering
	\includegraphics[width=\textwidth]{ImagesAmountPictures}
	\caption{Balkendiagramm Verteilung Bilder}
	% \todo{Quelle Bild!}
	\label{fig:barPics}
\end{figure}

Der \textit{Multi-Target-Tracking}-Algorithmus wurde benutzt um die Anzahl der Kugeln zu bestimmen.
Auf dem Förderband wurden 7712 Tracks von Kugeln,
7170 Tracks Pfefferkörnern,
19\,200 Tracks von Zylindern,
und 8702 Tracks von Weizenkörner zugeordnet.
Zudem wurden 5132 Tracks Kugeln und 3609 Tracks von Pfefferkörner auf der Rutsche zugeordnet.

Im DEM Datensatz sind die Tracks von 3713 Kugeln, 4357 Plättchen und 4427 Zylindern enthalten.
Die Verteilung der Tracks ist in Abbildung~\ref{fig:barTracks} zu sehen.

\begin{figure}[h]
	% \missingfigure{Visualisierung Dataaugmentation}
	\centering
	\includegraphics[width=\textwidth]{ImagesAmountTracks}
	\caption{Balkendiagramm Verteilung Tracks}
	% \todo{Quelle Bild!}
	\label{fig:barTracks}
\end{figure}

Die Feature-Label-Paare, die aus den Tracks extrahiert werden, 
müssen in ein Trainingsset und ein Testset aufgeteilt werden.
In manchen Situationen wird noch ein drittes Set, das so genannte Validationset, benötigt.
Dies ist zum Beispiel beim Hyperparameter Tuning der Fall.

Hierbei ist wichtig, dass das Testset eine gute Repräsentation des gesamten Datensets ist 
und ausreichend groß ist um statistisch aussagekräftige Ergebnisse zu produzieren.
Im Rahmen dieser Arbeit wurde meist eine Aufteilung im Verhältnis 90:10 zwischen Trainings- und Testset vorgenommen.
Es ist jedoch möglich beliebige andere Splits in der Hyperparameter Datei festzulegen.
% \todo{Vorgriff zum nächsten Kapitel - hier wissen wir noch gar nicht, dass ich ein Hyperparameter File habe...}  




% Train - Test - Validation - Split:
% Train - test, 90\% zu 10\%.
% Validation nur für die sets auf denen das Hyperparameter Tuning gemacht wurde
% [ungefähres ]

% \todo[inline]{Table mit Anzahl von Elementen in verschiedenen Batches?}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{img/scaledPieChart-trimmed}
%     \caption{Verteilung Schüttgut Elemente nach Sorte}
%     \label{piechartSchuettgut}
% \end{figure}



% Features:
% NextStep einfach alle \(n\)--Tupel, die ein Track hergibt, sodass es noch ein Label geben würde
% Separator: Muss unterschieden werden - Filtern oder nicht filtern, danach ob es das letzte mögliche Tupel vor der prediction Phase ist.
% Mit filtern besseres ergebnis, aber auch deutlich weniger Trainingsbeispiele (Overfitting wird mehr zur Gefahr)
% Ohne Filtern: Flexibler und mehr Trainingsbeispiele - man könnte im Nachhinen den PredictionCutOff verlegen 
% und einfach das Netz weiter verwenden ohne neu zu trainieren.
% Maybe ein Mittelding, das man nicht alle tupel nimmt aber auch nicht nur die letzten? Ausblick, zukunft



% Labels:
% Sehr straight forward für NextStep (Literally), einfach die nächste Zeile im Track jeweils für X und Y

% für separator slightly more complicated: 
% Element des Tracks vor und hinter der Separator position (entlang der Travel Achse)

% Schnittpunkt geometrisch bestimmen.
% Label für position ist die Position entlang der Achse orthogonal zur Bewegungsrichtung vom Schnittpunkt der Separatorlinie und 
% der Strecke zwischen dem Element vor und dem element hinter. siehe \ref{fig:Schnittpunkt}




% \todo{table of size of different data sets - number of pictures...}

% \color{blue}
% Verhältnis: Anzahl Feature-Label-Paare für verschiedene Beispiele und verschiedene Settings
% (FeatureSize, Filter Ja/Nein, Augmentation Ja/Nein) Als Tabelle?

% OUTDATED:
% Bei einer FeatureSize von 5 ergeben sich bei den Kugeln so 98.966 Feature-Label-Paare.
% Die Pfefferkörner haben dann 105.101 Feature-Le,
% bei den Zylindern kommt man auf 244.422 Feature-Label-Paare
% und bei den Weizenkörner 132.140 Feature-Label-Paare.
% \color{black}
